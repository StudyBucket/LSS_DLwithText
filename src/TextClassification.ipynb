{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Tutorial\n",
    "based on https://www.opencodez.com/python/text-classification-using-keras.htm\n",
    "\n",
    "- You can choose between two Data Sets\n",
    "    - Dataset ( http://archive.ics.uci.edu/ml/datasets/Reuter_50_50 ): contains documents of 50 different authors\n",
    "    - Dataset ( http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz ): contains document of 20 different newsgroup genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras import metrics\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "np.random.seed(1237)\n",
    " \n",
    "############ Newsgroup corpus ##############\n",
    "#path_train = \"./resources/20news-bydate/20news-bydate-train\"\n",
    "#path_test = \"./resources/20news-bydate/20news-bydate-test\"\n",
    "#num_labels = 20\n",
    "#test_files = [path_test + \"/sci.med/59225\",\n",
    "#             path_test + \"/sci.med/59229\",\n",
    "#             path_test + \"/rec.autos/103007\",\n",
    "#             path_test + \"/comp.graphics/38758\",\n",
    "#             path_test + \"/comp.graphics/38777\",]\n",
    "\n",
    "############ Reuter corpus ############## \n",
    "path_train = \"./resources/C50/C50train\"\n",
    "path_test = \"./resources/C50/C50test\"\n",
    "num_labels = 50\n",
    "\n",
    "test_files = [path_test + \"/LydiaZajc/45801newsML.txt\",\n",
    "             path_test + \"/LydiaZajc/377881newsML.txt\",\n",
    "            path_test + \"/LydiaZajc/416661newsML.txt\",\n",
    "            path_test + \"/FumikoFujisaki/416452newsML.txt\",\n",
    "            path_test + \"/KouroshKarimkhany/357767newsML.txt\"]\n",
    "\n",
    "\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_train = skds.load_files(path_train,load_content=False)\n",
    "\n",
    "file_paths = files_train.filenames #array with paths to every file\n",
    "label_names = files_train.target_names #all labels\n",
    "labelled_files_index = files_train.target #array with numerical represention of label for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tags = [\"filename\",\"category\",\"article\"]\n",
    "data_list = []\n",
    "\n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in file_paths:\n",
    "    data_list.append((f,\n",
    "                      label_names[labelled_files_index[i]],\n",
    "                      Path(f).read_text()))\n",
    "    i += 1\n",
    "    \n",
    "\n",
    "data = pd.DataFrame.from_records(data_list, columns=data_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "- we split the training data into 80/20\n",
    "- each element contains content, tag(category) and file name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take 80% of data for training, the remaining 20% vor test/evaluation\n",
    "train_size = int(len(data) * .8)\n",
    " \n",
    "train_posts = data['article'][:train_size]\n",
    "train_tags = data['category'][:train_size]\n",
    "train_files_names = data['filename'][:train_size]\n",
    " \n",
    "test_posts = data['article'][train_size:]\n",
    "test_tags = data['category'][train_size:]\n",
    "test_files_names = data['filename'][train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "- documents are modelled into tfidf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000 #vocabulary is restricted to 15000 words\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)#takes the most [vocab_size] frequent words\n",
    "tokenizer.fit_on_texts(train_posts)#selects features\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf') #weights features\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "- each label will be transformed in a binary vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "#Testprint for label vector\n",
    "print(\"Label: '\" + train_tags[0] + \"' will be transformed into:\")\n",
    "for i in range(num_labels):\n",
    "    print(encoder.classes_[i] + \": \" + str(y_train[0][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Data\n",
    "\n",
    "fitting/training of the model with training(X) & test(Y) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512)) \n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 5\n",
    "verbose = 1 #verbosity mode\n",
    "validation_split = 0.1\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', #loss function for multiclass classification\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=verbose,\n",
    "                    validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(x_test, y_test,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=1)\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting some unseen documents\n",
    "\n",
    "- files are taken from the test folder\n",
    "- steps: content tokenization, prediction, comparison with actual tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = []\n",
    "for t_f in test_files:\n",
    "    t_f_data = Path(t_f).read_text()\n",
    "    x_data.append(t_f_data)\n",
    " \n",
    "x_data_series = pd.Series(x_data)\n",
    "x_vectorized = tokenizer.texts_to_matrix(x_data_series, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for x_t in x_vectorized:\n",
    "    print(x_data[i][:80])\n",
    "    prediction = model.predict(np.array([x_t])) #returns propabilities for each label\n",
    "    predicted_label = encoder.classes_[np.argmax(prediction[0])] #chooses the most propable label\n",
    "    print(\"File ->\", test_files[i], \"Predicted label: \" + predicted_label)\n",
    "    print(\"********************************\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Data\n",
    "### Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_test = skds.load_files(path_test,load_content=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = files_test.filenames #array with paths to every file\n",
    "label_names = files_test.target_names #all labels\n",
    "labelled_files_index = files_test.target #array with numerical represention of label for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tags = [\"filename\",\"category\",\"article\"]\n",
    "data_list = []\n",
    "\n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in file_paths:\n",
    "    data_list.append((f,\n",
    "                      label_names[labelled_files_index[i]],\n",
    "                      Path(f).read_text()))\n",
    "    i += 1\n",
    "    \n",
    "\n",
    "data = pd.DataFrame.from_records(data_list, columns=data_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_posts = data['article']\n",
    "test_tags = data['category']\n",
    "test_files_names = data['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test = vectorizer.transform(test_posts)\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict(x_test) #matrix with probabilities for each label per document\n",
    "y_pred = np.argmax(probs, axis=1)#array with labels for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = confusion_matrix(y_pred, labelled_files_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = encoder.classes_\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cf)\n",
    "fig.colorbar(cax)\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation='vertical')\n",
    "ax.set_yticklabels(labels)\n",
    "#ax.set_yticklabels(np.arrage(len(test_tags)), test_tags)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
