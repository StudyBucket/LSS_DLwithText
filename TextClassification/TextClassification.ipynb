{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Tutorial\n",
    "based on https://www.opencodez.com/python/text-classification-using-keras.htm\n",
    "\n",
    "- We're about to classify newsgroup documents into 20 categories\n",
    "- Dataset (http://qwone.com/~jason/20Newsgroups/ 20newsbydate.tar.gz): contains training folder and test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "np.random.seed(1237)\n",
    " \n",
    "# Source file directory\n",
    "path_train = \"./resources/20news-bydate/20news-bydate-train\"\n",
    " \n",
    "files_train = skds.load_files(path_train,load_content=False)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = files_train.target\n",
    "label_names = files_train.target_names\n",
    "labelled_files = files_train.filenames\n",
    " \n",
    "data_tags = [\"filename\",\"category\",\"news\"]\n",
    "data_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],Path(f).read_text()))\n",
    "    i += 1\n",
    " \n",
    "# We have training data available as dictionary filename, category, data\n",
    "data = pd.DataFrame.from_records(data_list, columns=data_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "- we split the training data into 80/20\n",
    "- each element contains content, tag(category) and file name\n",
    "\n",
    "### Preprocessing\n",
    "- tokenization (keras Tokenizer) of the content of each document\n",
    "- tokenizer transforms each text in a vector by using tfidf weighting\n",
    "- encoding of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take 80% data as training and remaining 20% for test.\n",
    "train_size = int(len(data) * .8)\n",
    " \n",
    "train_posts = data['news'][:train_size]\n",
    "train_tags = data['category'][:train_size]\n",
    "train_files_names = data['filename'][:train_size]\n",
    " \n",
    "test_posts = data['news'][train_size:]\n",
    "test_tags = data['category'][train_size:]\n",
    "test_files_names = data['filename'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 news groups\n",
    "num_labels = 20\n",
    "vocab_size = 15000 #vocabulary is restricted to 15000 words\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)#welche WÃ¶rter\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    " \n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    " \n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Data\n",
    "\n",
    "- input layer: Dense (nodes = vocab_size, activation = sigmoid, dropout = 0.3)\n",
    "- hidden layer: Dense (nodes = 512, activation = sigmoid, dropout = 0.3)\n",
    "- output layer: Dense (nodes = 512, activation = softmax, dropout = 0.3)\n",
    "\n",
    "fitting/training of the model with training(X) & test(Y) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 300)               4500300   \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 20)                6020      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 4,596,620\n",
      "Trainable params: 4,596,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 11376 samples, validate on 1265 samples\n",
      "Epoch 1/10\n",
      "11376/11376 [==============================] - 28s 2ms/step - loss: 0.9579 - acc: 0.7486 - val_loss: 0.4092 - val_acc: 0.8901\n",
      "Epoch 2/10\n",
      "11376/11376 [==============================] - 22s 2ms/step - loss: 0.0821 - acc: 0.9810 - val_loss: 0.4304 - val_acc: 0.8941\n",
      "Epoch 3/10\n",
      "11376/11376 [==============================] - 22s 2ms/step - loss: 0.0366 - acc: 0.9945 - val_loss: 0.5474 - val_acc: 0.8838\n",
      "Epoch 4/10\n",
      "11376/11376 [==============================] - 22s 2ms/step - loss: 0.0243 - acc: 0.9966 - val_loss: 0.4971 - val_acc: 0.8949\n",
      "Epoch 5/10\n",
      "11376/11376 [==============================] - 22s 2ms/step - loss: 0.0136 - acc: 0.9982 - val_loss: 0.5635 - val_acc: 0.8885\n",
      "Epoch 6/10\n",
      "11376/11376 [==============================] - 24s 2ms/step - loss: 0.0196 - acc: 0.9970 - val_loss: 0.5548 - val_acc: 0.8814\n",
      "Epoch 7/10\n",
      "11376/11376 [==============================] - 24s 2ms/step - loss: 0.0154 - acc: 0.9976 - val_loss: 0.5917 - val_acc: 0.8909\n",
      "Epoch 8/10\n",
      "11376/11376 [==============================] - 24s 2ms/step - loss: 0.0340 - acc: 0.9955 - val_loss: 0.6781 - val_acc: 0.8672\n",
      "Epoch 9/10\n",
      "11376/11376 [==============================] - 23s 2ms/step - loss: 0.0306 - acc: 0.9952 - val_loss: 0.6288 - val_acc: 0.8696\n",
      "Epoch 10/10\n",
      "11376/11376 [==============================] - 21s 2ms/step - loss: 0.0168 - acc: 0.9974 - val_loss: 0.6271 - val_acc: 0.8783\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(vocab_size,)))#512\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(300)) #512\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting some unseen documents\n",
    "\n",
    "- files are taken from the test folder\n",
    "- steps: content tokenization, prediction, comparison with actual tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: kiwi@iis.ethz.ch (Rene Mueller)\n",
      "Subject: ICN (MSDOS) -> PBM/PGM/PPM format?\n",
      "Organization: Swiss Federal Institute of Technology (ETH), Zurich, CH\n",
      "Distribution: comp\n",
      "Lines: 7\n",
      "\n",
      "I have many icons in IconEdit and PBIcon format and I would like to \n",
      "convert them to PBM, PGM or PPM format. Do you know the formats of\n",
      "IconEdit or PBIcon?\n",
      "\n",
      "Thank's for your help.\n",
      "   ,\n",
      "Rene (kiwi@iis.ethz.ch)\n",
      "\n",
      "File -> ./resources/20news-bydate/20news-bydate-test/comp.graphics/38761 Predicted label: comp.windows.x\n",
      "********************************\n",
      "From: jack@acs2.bu.edu\n",
      "Subject: For Sale: Misc. Computer Parts & a radar detector\n",
      "Distribution: na\n",
      "Organization: Boston University, Boston, MA, USA\n",
      "Lines: 183\n",
      "Originator: jack@acs2.bu.edu\n",
      "\n",
      "\n",
      " I have the following computer items for sale:\n",
      "\n",
      "  Item                                Condition                 Price\n",
      "\n",
      " (a) Color EGA card and monitor            Working                 $180.00\n",
      "     Monitor made by Zenith\n",
      "     \n",
      "\n",
      " (b) (3)  1Mx8 80ns SIMMS by MT            Working                 $ 25.00/each\n",
      "     (16) 256K 80NS SIMMS by OKI           Working                 $  3.00/each\n",
      "     (4)  256K 100NS SIMMS by IT           Working                 $  2.75/each\n",
      "     (4)  256K 100NS SIMMS by Motorola     Working                 $  2.75/each\n",
      "     (4)  256K 120NS SIMMS by NEC          Working                 $  2.50/each\n",
      "\n",
      "     NOTE: All the above simms left over from  numerous Macintosh upgrade I\n",
      "           did over the years. If you have questions as to which one\n",
      "           fits your Mac, please mail me back.  \n",
      "\n",
      " (c) (2) AST FASTRAM card with 512K        Working                 $ 25.00/each\n",
      "         could be upgraded to 2M with\n",
      "         the DRAM in item(d)  \n",
      "\n",
      " (d) (54) 256K 100NS DRAM by TI            Working                 $  0.50/each\n",
      "     (36) 256K 100NS DRAM by OKI           Working                 $  0.50/each\n",
      "     (18) 256K 100NS DRAM by Hunydai       Working                 $  0.50/each\n",
      "\n",
      " (e) Compaq Classic \"Portable\" computer    Working                 $150.00\n",
      "     2 360K floppy drives\n",
      "     Amber 9 inch screen\n",
      "     256K RAM\n",
      "\n",
      " (f) (2) Serial card                       Working                 $ 10.00/each\n",
      " \n",
      " (g) Western Digital Disk Controller       Working                 $ 30.00\n",
      "     WD1003-WAH F003 X16\n",
      "     16 bit card for floopy\n",
      "     and harddisk\n",
      "\n",
      " (h) Logitech 3-buttons mechanical         Working                 $ 20.00\n",
      "     serial mouse\n",
      "     \n",
      "\n",
      " (i) (1) Full-size AT case with            Working                 $ 35.00\n",
      "         200W power-supply\n",
      "     \n",
      "     (1) Full-size AT case with            Working                 $ 60.00\n",
      "         200W power-supply \n",
      "         This is the original\n",
      "         case for my AST Premium/286\n",
      "         computer, it could have\n",
      "         up to 5 half-height devices\n",
      "         (three of which could be\n",
      "         floopy drives, tape drive etc).\n",
      "         I am also including the original\n",
      "         286 motherboard which condition\n",
      "         is unknown.\n",
      "\n",
      " (j) (2) 1.2M 5.25\" floopy drive         Working                  $ 30.00/each\n",
      "     (1) 1.44 3.5\" floopy drive          Working                  $ 38.00\n",
      "   \n",
      " (k) Adaptec 1542B SCSI adapter          Brand New                $220.00\n",
      "\n",
      " (l) Wangtek 5150ES SCSI 250M            Working                  $200.00\n",
      "     1/4\" tape drive    \n",
      "\n",
      " (m) 1/2 height 40M MFM drive            Working                  $100.00\n",
      "     by Miniscribe?\n",
      "     1/2 height 40M MFM drive            Working                  $100.00\n",
      "     by Seagate ST 251-1         \n",
      "     1/2 height 20M MFM drive            Working                  $ 50.00\n",
      "     by Seagate\n",
      "\n",
      " (n) Prodigy start-up kit for            Brand New                $ 45.00\n",
      "     PC with 24/96 data/fax\n",
      "     modem\n",
      "     \n",
      " The following items I am selling as is, all the them are in unknown\n",
      " condition, either I never get it to work or never try to hook it up.\n",
      "\n",
      " (o) EGA card                            Unknown                  $  5.00\n",
      "\n",
      " (p) Multi-function game/clock/          Unknown                  $  5.00\n",
      "     parallel/serial port card\n",
      "\n",
      " (q) Monochrome Graphics 132 columns     Unknown                  $  5.00\n",
      "     graphics card  \n",
      "\n",
      " (r) CDC 94171-344 340M SCSI drive       Unknown                  $150.00\n",
      "\n",
      " (q) Miniscribe 20M SCSI drive           Unknown                  $ 15.00\n",
      "\n",
      " (r) Prodigy start-up kit                Unknown                  FREE\n",
      "\n",
      " (s) (2) AST-3G Plus Chip                Unknown                  $  2.00\n",
      "\n",
      " (t) Seagate 80M MFM drive               Unknown                  $ 80.00\n",
      "     model ST-4096 \n",
      "\n",
      " I bought the radar detector a couple years ago for obvious reason(s)\n",
      " and I have never been ticketed for the past 3 years and now I don't\n",
      " drive to work anymore so I would rather sell it.\n",
      "\n",
      " (u) Whristler 425 radar detector        Working                 $ 30.00\n",
      "     X & K bands   \n",
      "\n",
      "\n",
      " If you would like to buy any of the above items, please mail me\n",
      " at jack@acs.bu.edu. Also, a 10% automatic discount will apply if\n",
      " your total purchase price is $100 or more (except the two brand\n",
      " new items). Buyer(s) pay shipping.\n",
      " \n",
      " If you think the prices I listed above is unrealistic, please mail\n",
      " me back and I would take your advice into consideration and make the\n",
      " proper adjustments.\n",
      "\n",
      " The reason I am selling this stuff is because I have decided that\n",
      " I had enough with this hobby of PC computing and I want to move on\n",
      " other interests.\n",
      "\n",
      " One last thing, if you know any non-profit organization whom might\n",
      " be interested in my equipments, please let me know. Because if no\n",
      " one wants to buy them, I might as well donate them and get a tax\n",
      " break...\n",
      "\n",
      " Thanks.\n",
      "\n",
      "\t\t\t\t\t\t\t-Jack\n",
      "\n",
      "\t\t\t\t\t\t\tjack@acs.bu.edu\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-- \n",
      "\n",
      "*******************************************************************\n",
      "* BITNet : ccjcc@buacca     \\  Jack C. Chan @ Boston University   *\n",
      "* ARPA : jack@bu-it.bu.edu   \\  Internet : jack@bu-it.bu.edu      *\n",
      "\n",
      "File -> ./resources/20news-bydate/20news-bydate-test/misc.forsale/76117 Predicted label: misc.forsale\n",
      "********************************\n",
      "From: wjhovi01@ulkyvx.louisville.edu\n",
      "Subject: Re: Homosexuality issues in Christianity\n",
      "Organization: University of Louisville\n",
      "Lines: 19\n",
      "\n",
      "bruce@liv.ac.uk (Bruce Stephens) suggests different levels of acceptance of\n",
      "homosexuality:\n",
      "> \n",
      "> 1) Regard homosexual orientation as a sin (or evil, whatever)\n",
      "> 2) Regard homosexual behaviour as a sin, but accept orientation\n",
      "> (though presumably orientation is unfortunate) and dislike people who\n",
      "> indulge\n",
      "> 3) As 2, but \"love the sinner\"\n",
      "> 4) Accept homosexuality altogether.\n",
      "\n",
      "I would add 4': our churches should accept homosexual orientation but hold all\n",
      "people to certain standards of sexual behavior.  Promiscuity, abuse of power\n",
      "relationships, harrassment, compulsivity are equally out of place in the lives\n",
      "of homosexual as of heterosexual people.\n",
      "\n",
      "Of course, this would bring up the dread shibboleth of homosexual marriage,\n",
      "and we couldn't have that! :-)\n",
      "\n",
      "billh\n",
      "\n",
      "File -> ./resources/20news-bydate/20news-bydate-test/soc.religion.christian/21411 Predicted label: soc.religion.christian\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "# These are the labels we stored from our training\n",
    "# The order is very important here.\n",
    " \n",
    "labels = np.array(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x',\n",
    " 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball',\n",
    " 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space',\n",
    " 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast',\n",
    " 'talk.politics.misc', 'talk.religion.misc'])\n",
    " \n",
    "test_files = [\"./resources/20news-bydate/20news-bydate-test/comp.graphics/38761\",\n",
    "              \"./resources/20news-bydate/20news-bydate-test/misc.forsale/76117\",\n",
    "              \"./resources/20news-bydate/20news-bydate-test/soc.religion.christian/21411\"\n",
    "              ]\n",
    "x_data = []\n",
    "for t_f in test_files:\n",
    "    t_f_data = Path(t_f).read_text()\n",
    "    x_data.append(t_f_data)\n",
    " \n",
    "x_data_series = pd.Series(x_data)\n",
    "x_tokenized = tokenizer.texts_to_matrix(x_data_series, mode='tfidf')\n",
    " \n",
    "i=0\n",
    "for x_t in x_tokenized:\n",
    "    print(x_data[i])\n",
    "    prediction = model.predict(np.array([x_t]))\n",
    "    predicted_label = labels[np.argmax(prediction[0])]\n",
    "    print(\"File ->\", test_files[i], \"Predicted label: \" + predicted_label)\n",
    "    print(\"********************************\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
