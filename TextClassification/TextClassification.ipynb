{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johanna\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification Tutorial\n",
    "based on https://www.opencodez.com/python/text-classification-using-keras.htm\n",
    "\n",
    "- We're about to classify newsgroup documents into 20 categories\n",
    "- Dataset (http://qwone.com/~jason/20Newsgroups/ 20newsbydate.tar.gz): contains training folder and test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "np.random.seed(1237)\n",
    " \n",
    "# Source file directory\n",
    "path_train = \"./resources/20news-bydate/20news-bydate-train\"\n",
    " \n",
    "files_train = skds.load_files(path_train,load_content=False)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = files_train.target\n",
    "label_names = files_train.target_names\n",
    "labelled_files = files_train.filenames\n",
    " \n",
    "data_tags = [\"filename\",\"category\",\"news\"]\n",
    "data_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],Path(f).read_text()))\n",
    "    i += 1\n",
    " \n",
    "# We have training data available as dictionary filename, category, data\n",
    "data = pd.DataFrame.from_records(data_list, columns=data_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we split the training data into 80/20\n",
    "- each element contains content, tag(category) and file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take 80% data as training and remaining 20% for test.\n",
    "train_size = int(len(data) * .8)\n",
    " \n",
    "train_posts = data['news'][:train_size]\n",
    "train_tags = data['category'][:train_size]\n",
    "train_files_names = data['filename'][:train_size]\n",
    " \n",
    "test_posts = data['news'][train_size:]\n",
    "test_tags = data['category'][train_size:]\n",
    "test_files_names = data['filename'][train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "\n",
    "- tokenization (keras Tokenizer) of the content of each document\n",
    "- tokenizer transforms each text in a vector by using tfidf weighting\n",
    "- encoding of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 news groups\n",
    "num_labels = 20\n",
    "vocab_size = 15000 #vocabulary is restricted to 15000 words\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    " \n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    " \n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder = LabelBinarizer()\n",
    "#encoder.fit(train_tags)\n",
    "#y_train = encoder.transform(train_tags)\n",
    "#y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model\n",
    "\n",
    "- input layer: Dense (nodes = vocab_size, activation = sigmoid, dropout = 0.3)\n",
    "- hidden layer: Dense (nodes = 512, activation = sigmoid, dropout = 0.3)\n",
    "- hidden layer: Dense (nodes = 512, activation = softmax, dropout = 0.3)\n",
    "- output layer: nodes = num_labels\n",
    "\n",
    "fitting/training of the model with training(X) & test(Y) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               7680512   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 7,953,428\n",
      "Trainable params: 7,953,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8145 samples, validate on 906 samples\n",
      "Epoch 1/10\n",
      "8145/8145 [==============================] - 31s 4ms/step - loss: 1.7572 - acc: 0.5638 - val_loss: 0.5918 - val_acc: 0.8896\n",
      "Epoch 2/10\n",
      "8145/8145 [==============================] - 27s 3ms/step - loss: 0.2794 - acc: 0.9500 - val_loss: 0.3537 - val_acc: 0.9106\n",
      "Epoch 3/10\n",
      "8145/8145 [==============================] - 27s 3ms/step - loss: 0.0828 - acc: 0.9887 - val_loss: 0.3065 - val_acc: 0.9139\n",
      "Epoch 4/10\n",
      "8145/8145 [==============================] - 27s 3ms/step - loss: 0.0367 - acc: 0.9963 - val_loss: 0.2953 - val_acc: 0.9194\n",
      "Epoch 5/10\n",
      "8145/8145 [==============================] - 28s 3ms/step - loss: 0.0200 - acc: 0.9983 - val_loss: 0.3029 - val_acc: 0.9150\n",
      "Epoch 6/10\n",
      "8145/8145 [==============================] - 28s 3ms/step - loss: 0.0133 - acc: 0.9985 - val_loss: 0.3031 - val_acc: 0.9172\n",
      "Epoch 7/10\n",
      "8145/8145 [==============================] - 28s 3ms/step - loss: 0.0089 - acc: 0.9988 - val_loss: 0.3102 - val_acc: 0.9117\n",
      "Epoch 8/10\n",
      "8145/8145 [==============================] - 29s 4ms/step - loss: 0.0089 - acc: 0.9988 - val_loss: 0.3100 - val_acc: 0.9161\n",
      "Epoch 9/10\n",
      "8145/8145 [==============================] - 29s 4ms/step - loss: 0.0058 - acc: 0.9991 - val_loss: 0.3177 - val_acc: 0.9172\n",
      "Epoch 10/10\n",
      "8145/8145 [==============================] - 28s 3ms/step - loss: 0.0061 - acc: 0.9989 - val_loss: 0.3114 - val_acc: 0.9183\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting some unseen documents\n",
    "\n",
    "- files are taken from the test folder\n",
    "- content tokenization, prediction, comparison with actual tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File -> ./resources/20news-bydate/20news-bydate-test/comp.graphics/38761 Predicted label: comp.graphics\n",
      "File -> ./resources/20news-bydate/20news-bydate-test/misc.forsale/76117 Predicted label: misc.forsale\n",
      "File -> ./resources/20news-bydate/20news-bydate-test/soc.religion.christian/21411 Predicted label: soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "# These are the labels we stored from our training\n",
    "# The order is very important here.\n",
    " \n",
    "labels = np.array(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x',\n",
    " 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball',\n",
    " 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space',\n",
    " 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast',\n",
    " 'talk.politics.misc', 'talk.religion.misc'])\n",
    " \n",
    "test_files = [\"./resources/20news-bydate/20news-bydate-test/comp.graphics/38761\",\n",
    "              \"./resources/20news-bydate/20news-bydate-test/misc.forsale/76117\",\n",
    "              \"./resources/20news-bydate/20news-bydate-test/soc.religion.christian/21411\"\n",
    "              ]\n",
    "x_data = []\n",
    "for t_f in test_files:\n",
    "    t_f_data = Path(t_f).read_text()\n",
    "    x_data.append(t_f_data)\n",
    " \n",
    "x_data_series = pd.Series(x_data)\n",
    "x_tokenized = tokenizer.texts_to_matrix(x_data_series, mode='tfidf')\n",
    " \n",
    "i=0\n",
    "for x_t in x_tokenized:\n",
    "    prediction = model.predict(np.array([x_t]))\n",
    "    predicted_label = labels[np.argmax(prediction[0])]\n",
    "    print(\"File ->\", test_files[i], \"Predicted label: \" + predicted_label)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
